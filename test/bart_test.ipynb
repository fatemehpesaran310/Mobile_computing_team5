{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/BritneyMuller/colab-notebooks/blob/master/Easy_Text_Summarization_with_BART.ipynb#scrollTo=-DcmOk-0UPvv\n",
    "\n",
    "Code taken and/or modified from colab file released by google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kims90/virtual_envs/mobile_prj/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart_summarize(text, num_beams, length_penalty, max_length, min_length, no_repeat_ngram_size):\n",
    "\n",
    "  text = text.replace('\\n','')\n",
    "  text_input_ids = tokenizer.batch_encode_plus([text], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "  summary_ids = model.generate(text_input_ids, num_beams=int(num_beams), length_penalty=float(length_penalty), max_length=int(max_length), min_length=int(min_length), no_repeat_ngram_size=int(no_repeat_ngram_size))           \n",
    "  summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "  return summary_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/kims90/.cache/huggingface/datasets/kmfoda___csv/kmfoda--booksum-025141c210e07407/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 317.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle \n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"kmfoda/booksum\")\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {'rouge-1': {'r': np.zeros((len(dataset[\"test\"], ))), 'p': np.zeros((len(dataset[\"test\"], ))), 'f': np.zeros((len(dataset[\"test\"], )))}, \n",
    "               'rouge-2': {'r': np.zeros((len(dataset[\"test\"], ))), 'p': np.zeros((len(dataset[\"test\"], ))), 'f': np.zeros((len(dataset[\"test\"], )))}, \n",
    "               'rouge-l': {'r': np.zeros((len(dataset[\"test\"], ))), 'p': np.zeros((len(dataset[\"test\"], ))), 'f': np.zeros((len(dataset[\"test\"], )))}, }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All states, all powers, that have held and hold rule over men have beenand are either republics or principalities.Principalities are either hereditary, in which the family has been longestablished; or they are new. I will leave out all discussion on republics, inasmuch as in anotherplace I have written of them at length.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bart_summarize(dataset[\"test\"][\"chapter\"][0], 4, 2.0, 142, 56, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    print(model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All states, all powers, that have held and hold rule over men have beenand are either republics or principalities.Principalities are either hereditary, in which the family has been longestablished; or they are new. I will leave out all discussion on republics, inasmuch as in anotherplace I have written of them at length.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_summarize(dataset[\"test\"][\"chapter\"][0], 4, 2.0, 142, 56, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 34/1431 [00:25<17:43,  1.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]))):\n\u001b[0;32m----> 2\u001b[0m     hypothesis \u001b[39m=\u001b[39m bart_summarize(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mchapter\u001b[39;49m\u001b[39m\"\u001b[39;49m][i], \u001b[39m4\u001b[39;49m, \u001b[39m2.0\u001b[39;49m, \u001b[39m142\u001b[39;49m, \u001b[39m56\u001b[39;49m, \u001b[39m3\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     reference \u001b[39m=\u001b[39m dataset[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msummary_text\u001b[39m\u001b[39m\"\u001b[39m][i]\n\u001b[1;32m      4\u001b[0m     scores \u001b[39m=\u001b[39m rouge\u001b[39m.\u001b[39mget_scores(hypothesis, reference)\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mbart_summarize\u001b[0;34m(text, num_beams, length_penalty, max_length, min_length, no_repeat_ngram_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m text_input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus([text], return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(torch_device)\n\u001b[0;32m----> 5\u001b[0m summary_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(text_input_ids, num_beams\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(num_beams), length_penalty\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(length_penalty), max_length\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(max_length), min_length\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(min_length), no_repeat_ngram_size\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(no_repeat_ngram_size))           \n\u001b[1;32m      6\u001b[0m summary_txt \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(summary_ids\u001b[39m.\u001b[39msqueeze(), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m summary_txt\n",
      "File \u001b[0;32m~/virtual_envs/mobile_prj/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/virtual_envs/mobile_prj/lib/python3.8/site-packages/transformers/generation/utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1596\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1597\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1598\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1599\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1600\u001b[0m     )\n\u001b[1;32m   1601\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1603\u001b[0m         input_ids,\n\u001b[1;32m   1604\u001b[0m         beam_scorer,\n\u001b[1;32m   1605\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1606\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1607\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1608\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1609\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1610\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1611\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1612\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1613\u001b[0m     )\n\u001b[1;32m   1615\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1616\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1617\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/virtual_envs/mobile_prj/lib/python3.8/site-packages/transformers/generation/utils.py:2945\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2942\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores\u001b[39m.\u001b[39mview(batch_size, num_beams \u001b[39m*\u001b[39m vocab_size)\n\u001b[1;32m   2944\u001b[0m \u001b[39m# Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)\u001b[39;00m\n\u001b[0;32m-> 2945\u001b[0m next_token_scores, next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtopk(\n\u001b[1;32m   2946\u001b[0m     next_token_scores, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m num_beams, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, largest\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39msorted\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m   2947\u001b[0m )\n\u001b[1;32m   2949\u001b[0m next_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdiv(next_tokens, vocab_size, rounding_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2950\u001b[0m next_tokens \u001b[39m=\u001b[39m next_tokens \u001b[39m%\u001b[39m vocab_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset[\"test\"]))):\n",
    "    hypothesis = bart_summarize(dataset[\"test\"][\"chapter\"][i], 4, 2.0, 142, 56, 3)\n",
    "    reference = dataset[\"test\"][\"summary_text\"][i]\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "\n",
    "    for j in scores_dict.keys():\n",
    "        for k in scores_dict[j].keys():\n",
    "            scores_dict[j][k][i] = scores[0][j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.108416854423227"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores_dict['rouge-1']['r'][:34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarize_bart_param(tokenizer, model, text, device, num_beams, length_penalty, max_length, min_length, no_repeat_ngram_size, early_stopping):\n",
    "\n",
    "  text = text.replace('\\n','')\n",
    "  text_input_ids = tokenizer.batch_encode_plus([text], return_tensors='pt', max_length=512, truncation=True, padding=True)['input_ids'].to(device)\n",
    "  summary_ids = model.generate(text_input_ids, num_beams=int(num_beams), length_penalty=float(length_penalty), max_length=int(max_length), min_length=int(min_length), no_repeat_ngram_size=int(no_repeat_ngram_size), early_stopping=early_stopping)           \n",
    "  summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "  return summary_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Do you not want to know who has taken it?” cried his wife impatiently. “You want to tell me, and I have no objection to hearing it.”“Why, my dear, you must know, Mrs. Long says that Netherfield istaken by a young man of large fortune from the north of England”'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"../source_text/pride_and_prejudice_ch1_part.txt\", \"r\")\n",
    "q_text = f.read()\n",
    "\n",
    "result = text_summarize_bart_param(tokenizer, model, q_text, 'cuda', 4, 2.0, 142, 56, 3, True)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supermarket chain Lidl said it had sold enough bunting to line the Coronation procession route 75 times over. Tesco said it was on track to sell 675,000 pork pies and 300,000 pots of clotted cream. Around £200m will be spent on food and drink this weekend, according to Centre for Retail Research.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"../source_text/bbc_news_part.txt\", \"r\")\n",
    "b_text = f.read()\n",
    "\n",
    "result = text_summarize_bart_param(tokenizer, model, b_text, 'cuda', 4, 2.0, 142, 56, 3, True)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile_prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
